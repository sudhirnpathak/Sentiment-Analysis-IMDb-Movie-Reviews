{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDb Movie Reviews\n",
    "\n",
    "In this notebook, we learn sentiment analysis, focusing on classification of IMDb movie reviews. \n",
    "<br>In the first part, we implement very basic text preprocessing and develop a base-line model using logistic regression.\n",
    "<br>Then, we improve the model by implementing several advanced text preprocessing techniques used to enhance NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sudhir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sudhir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sudhir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "The IMDb movie review data set can be found at http://ai.stanford.edu/~amaas/data/sentiment/. \n",
    "\n",
    "The dataset has 50k movie reviews, of which we use 25k for training the model and rest 25k for testing the model. <br>Each set has 12.5k positive reviews and 12.5k negative reviews. \n",
    "\n",
    "In IMDb, movies are rated from 1-10. In the dataset, the movies with ratings <= 4 are rated negative and the ones with >=7 are rated positive. \n",
    "<br>Movies with ratings = 5,6 are omitted while creating this dataset.\n",
    "\n",
    "We have merged all the training/test set reviews into the files full_train.txt/full_test.txt respectively. \n",
    "<br>In each file, first 12.5k are positive reviews and the rest 12.5k are negative reviews\n",
    "\n",
    "We put all the reviews in two lists, one for training and the other for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_raw = []\n",
    "for line in open('./movie_data/full_train.txt', 'r'):\n",
    "    reviews_train_raw.append(line.strip())\n",
    "    \n",
    "reviews_test_raw = []\n",
    "for line in open('./movie_data/full_test.txt', 'r'):\n",
    "    reviews_test_raw.append(line.strip())\n",
    "    \n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_train_raw), len(reviews_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text preprocessing & creating baseline model\n",
    "\n",
    "**Tokenize**\n",
    "\n",
    "In the first step we tokenize the reviews. Let's have a look at one of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tokenization step, we implement some basic text processing as we aim to create a base-line model. \n",
    " - making all words lower case\n",
    " - considering words which consist of alphabets only (. , : ! ? 10 w3p kind of entries removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    \n",
    "    review_tokenize = []\n",
    "    \n",
    "    for review in corpus:\n",
    "        \n",
    "        tokenized = ' '.join([word for word in word_tokenize(review.lower()) if word.isalpha()])\n",
    "        review_tokenize.append(tokenized)\n",
    "        \n",
    "    return review_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = tokenize(reviews_train_raw)\n",
    "reviews_test = tokenize(reviews_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my years in the teaching profession lead me to believe that bromwell high satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it is'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization**\n",
    "\n",
    "Now, in order to feed the text data to the algorithm, we have to convert it to numerical representation. For this purpose, we use one-hot encoding. \n",
    "\n",
    "In this representation, we create a very large matrix where \n",
    " - the number of rows is the number of reviews and \n",
    " - the number of columns is the number of unique words in our dataset (called corpus)\n",
    " \n",
    "For every row (review), if the corresponding word in the column exists in the review it is set to be 1, otherwise 0. The end result is a sparse matrix of few 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(reviews_train)\n",
    "X = cv.transform(reviews_train)\n",
    "X_test = cv.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set one-hot shape: (25000, 71400)\n",
      "Test set one-hot shape: (25000, 71400)\n"
     ]
    }
   ],
   "source": [
    "print('Train set one-hot shape:', X.shape)\n",
    "print('Test set one-hot shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training set of 25k reviews, we have around 71k unique words. Note that the one-hot encoding is done on the train set only. If a word appears in the test set that did not exist in the corpus of train set, it is of no consequence, as it wont find its place in the one-hot encoding represenation.\n",
    "\n",
    "Lets have a look at the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aaaaaaah', 'aaaaah', 'aaaahhhhhhh', 'aaaand', 'aaaarrgh', 'aaah', 'aaargh', 'aaaugh', 'aachen', 'aada', 'aadha', 'aag', 'aage', 'aaghh', 'aah', 'aahhh', 'aaip', 'aaja', 'aakash', 'aaker', 'aaliyah', 'aames', 'aamir', 'aan', 'aankh', 'aankhen', 'aap', 'aapke', 'aapkey', 'aardman', 'aardvarks', 'aargh', 'aaron', 'aarp', 'aarrrgh', 'aatish', 'aauugghh', 'aavjo', 'aaww', 'ab', 'aback', 'abahy', 'abanazer', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abandons', 'abanks', 'abas', 'abashed', 'abashidze', 'abatement', 'abating', 'abattoirs', 'abba', 'abbad', 'abbas', 'abbasi', 'abbey', 'abbie', 'abbot', 'abbots', 'abbott', 'abbreviated', 'abbu', 'abby', 'abc', 'abcd', 'abdic', 'abdicates', 'abdicating', 'abdomen', 'abdominal', 'abduct', 'abducted', 'abductee', 'abducting', 'abduction', 'abductions', 'abductor', 'abductors', 'abducts', 'abdul', 'abdullah', 'abe', 'abel', 'abercrombie', 'abernathy', 'aberrant', 'aberration', 'aberrations', 'aberystwyth', 'abets', 'abetted', 'abetting', 'abeyance', 'abgail']\n"
     ]
    }
   ],
   "source": [
    "features = cv.get_feature_names()\n",
    "print(features[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Model**\n",
    "\n",
    "To create the base-line classification model, we use logistic regression because\n",
    " - linear models tends to perform well on sparse datasets\n",
    " - easy to interpret\n",
    " - it is fast compared to other complicated algorithms\n",
    "\n",
    "The target variable remains the same for both training and test set, as in both the first 12.5k reviews are positive and rest 12.5k are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set with C = 1.00: 0.998/0.878\n",
      "Accuracy on train/validation set with C = 0.50: 0.994/0.882\n",
      "Accuracy on train/validation set with C = 0.25: 0.986/0.883\n",
      "Accuracy on train/validation set with C = 0.05: 0.950/0.889\n",
      "Accuracy on train/validation set with C = 0.01: 0.905/0.879\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "\n",
    "for c in [1, 0.5, 0.25, 0.05, 0.01]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c)       # hyperparameter C for regularization\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy on train/validation set with C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                  accuracy_score(y_train, lr.predict(X_train)), \n",
    "                                                  accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train/test set: 0.947/0.881\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 0.05)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train/test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                   accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with our simple baseline model we have 88% accuracy.\n",
    "\n",
    "Now, as a sanity check, lets look at the five largest and smallest coefficients to get the five most discriminating words for both positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[-0.01020906 -0.0125288  -0.0002121  ...  0.00025972 -0.02385085\n",
      "  -0.01536539]]\n",
      "Number of coefficients: 71400\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients:', best_model.coef_)\n",
    "print('Number of coefficients:', len(best_model.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words\n",
      "('excellent', 0.9462702638847761)\n",
      "('perfect', 0.7550828420618055)\n",
      "('great', 0.6830621779003093)\n",
      "('amazing', 0.6364490924130619)\n",
      "('superb', 0.6296931914827026)\n",
      "--------------\n",
      "Negative words\n",
      "('worst', -1.400142312303066)\n",
      "('waste', -1.1860009859358518)\n",
      "('awful', -0.9809824977718387)\n",
      "('poorly', -0.875555391069068)\n",
      "('boring', -0.8217719508421568)\n"
     ]
    }
   ],
   "source": [
    "feature_coeff = {word: coeff for word, coeff in zip(cv.get_feature_names(), best_model.coef_[0])}\n",
    "\n",
    "print('Positive words')\n",
    "for best_positive in sorted(feature_coeff.items(), key = lambda x: x[1], reverse = True)[:5]:\n",
    "    print(best_positive)\n",
    "\n",
    "print('--------------')\n",
    "    \n",
    "print('Negative words')    \n",
    "for best_negative in sorted(feature_coeff.items(), key = lambda x: x[1])[:5]:\n",
    "    print(best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top five words in both positive and negative cases are easily identifiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced text preprocessing and modeling\n",
    "\n",
    "Now, lets improve the model by implementing few advanced text preprocessing techniques.\n",
    "<br>In particular, we consider these four options:\n",
    "\n",
    " - Removing stop words\n",
    " - Normalization: Stemming and Lemmatization\n",
    " - N-grams\n",
    " - Representations: word counts and tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. REMOVE STOP WORDS**\n",
    "\n",
    "We further process the text by removing very common words (called stop words) such as 'if, we, us, he, she. \n",
    "<br>We can usually remove these words without changing the semantics of a text, and doing so often improves the performance of a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustration of a review before removing the stop words\n",
      "Number of words: 137\n",
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my years in the teaching profession lead me to believe that bromwell high satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it is\n"
     ]
    }
   ],
   "source": [
    "print('Illustration of a review before removing the stop words')\n",
    "print('Number of words:', len(reviews_train[0].split()))\n",
    "print(reviews_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(corpus):\n",
    "    \n",
    "    english_stop_words = stopwords.words('english')\n",
    "    \n",
    "    without_stop_words = []\n",
    "    \n",
    "    for review in corpus:\n",
    "        \n",
    "        cleaned = ' '.join([word for word in review.split() if word not in english_stop_words])\n",
    "        without_stop_words.append(cleaned)\n",
    "        \n",
    "    return without_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set with C = 1.00: 0.998/0.874\n",
      "Accuracy on train/validation set with C = 0.50: 0.994/0.877\n",
      "Accuracy on train/validation set with C = 0.25: 0.986/0.878\n",
      "Accuracy on train/validation set with C = 0.05: 0.949/0.882\n",
      "Accuracy on train/validation set with C = 0.01: 0.906/0.873\n"
     ]
    }
   ],
   "source": [
    "reviews_train_no_stop_words = remove_stop_words(reviews_train)\n",
    "reviews_test_no_stop_words = remove_stop_words(reviews_test)\n",
    "\n",
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(reviews_train_no_stop_words)\n",
    "X = cv.transform(reviews_train_no_stop_words)\n",
    "X_test = cv.transform(reviews_test_no_stop_words)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "\n",
    "for c in [1, 0.5, 0.25, 0.05, 0.01]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy on train/validation set with C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                 accuracy_score(y_train, lr.predict(X_train)),\n",
    "                                                 accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train/test set: 0.946/0.880\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 0.05)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train/test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                      accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustration of a review after removing the stop words\n",
      "Number of words: 69\n",
      "bromwell high cartoon comedy ran time programs school life teachers years teaching profession lead believe bromwell high satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity\n"
     ]
    }
   ],
   "source": [
    "print('Illustration of a review after removing the stop words')\n",
    "print('Number of words:', len(reviews_train_no_stop_words[0].split()))\n",
    "print(reviews_train_no_stop_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords does not signal anything about a movie review being positive or negative and are unnecessary burden, increasing the dimension of one-hot encoding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 71262)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words does not seem to have making any improvements in accuracy. It could be because we have removed very few stop words, as the number of unique words are down to 71262 only from 71400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. NORMALIZATION**\n",
    "\n",
    "A common next step in text preprocessing is normalization: converting all of the different forms of a given word into one. \n",
    "<br> There are two methods for this: Stemming and Lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Stemming**\n",
    "\n",
    "Stemming is considered to be a brute-force approach to normalization. There are several algorithms for stemming, and in general they all use basic rules to chop off the ends of words. NLTK has several stemming algorithm implementation, and we use the Porter stemmer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustration of a review before stemming\n",
      "Number of words: 137\n",
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my years in the teaching profession lead me to believe that bromwell high satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it is\n"
     ]
    }
   ],
   "source": [
    "print('Illustration of a review before stemming')\n",
    "print('Number of words:', len(reviews_train[0].split()))\n",
    "print(reviews_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(corpus):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_reviews = []\n",
    "    \n",
    "    for review in corpus:\n",
    "        \n",
    "        stemmed = ' '.join([stemmer.stem(word) for word in review.split()])\n",
    "        stemmed_reviews.append(stemmed)\n",
    "    \n",
    "    return stemmed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set for C = 1.00: 0.994/0.868\n",
      "Accuracy on train/validation set for C = 0.50: 0.986/0.870\n",
      "Accuracy on train/validation set for C = 0.25: 0.972/0.871\n",
      "Accuracy on train/validation set for C = 0.05: 0.937/0.874\n",
      "Accuracy on train/validation set for C = 0.01: 0.901/0.868\n"
     ]
    }
   ],
   "source": [
    "reviews_train_stemmed = stem(reviews_train)\n",
    "reviews_test_stemmed = stem(reviews_test)\n",
    "\n",
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(reviews_train_stemmed)\n",
    "X = cv.transform(reviews_train_stemmed)\n",
    "X_test = cv.transform(reviews_test_stemmed)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "\n",
    "for c in [1, 0.5, 0.25, 0.05, 0.01]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy on train/validation set for C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                 accuracy_score(y_train, lr.predict(X_train)),\n",
    "                                                 accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train/test set: 0.969/0.869\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 0.25)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train/test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                       accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 48439)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique words have drastically reduced from 71k to 48k, and the accuracy is suprisingly down, 0.869 from 0.881. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustration of a review after stemming\n",
      "Number of words: 137\n",
      "bromwel high is a cartoon comedi it ran at the same time as some other program about school life such as teacher my year in the teach profess lead me to believ that bromwel high satir is much closer to realiti than is teacher the scrambl to surviv financi the insight student who can see right through their pathet teacher pomp the petti of the whole situat all remind me of the school i knew and their student when i saw the episod in which a student repeatedli tri to burn down the school i immedi recal at high a classic line inspector i here to sack one of your teacher student welcom to bromwel high i expect that mani adult of my age think that bromwel high is far fetch what a piti that it is\n"
     ]
    }
   ],
   "source": [
    "print('Illustration of a review after stemming')\n",
    "print('Number of words:', len(reviews_train_stemmed[0].split()))\n",
    "print(reviews_train_stemmed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the effect of stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "bromwell bromwel\n",
      "comedy comedi\n",
      "programs program\n",
      "teachers teacher\n",
      "years year\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "\n",
    "for before, after in zip(reviews_train[0].split(), reviews_train_stemmed[0].split()):\n",
    "    if after != before:\n",
    "        differences.append([before, after])\n",
    "\n",
    "print(len(differences))\n",
    "for i in range(5):\n",
    "    print(differences[i][0], differences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Lemmatization**\n",
    "\n",
    "Lemmatization identifies the part-of-speech of a given word and then applies more complex rules to transform the word into its true root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(corpus):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_reviews = []\n",
    "    \n",
    "    for review in corpus:\n",
    "               \n",
    "        lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in review.split()])\n",
    "        lemmatized_reviews.append(lemmatized)\n",
    "        \n",
    "    return lemmatized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set for C = 1.00: 0.997/0.871\n",
      "Accuracy on train/validation set for C = 0.50: 0.993/0.872\n",
      "Accuracy on train/validation set for C = 0.25: 0.983/0.877\n",
      "Accuracy on train/validation set for C = 0.05: 0.946/0.876\n",
      "Accuracy on train/validation set for C = 0.01: 0.904/0.867\n"
     ]
    }
   ],
   "source": [
    "reviews_train_lemmatized = lemmatize(reviews_train)\n",
    "reviews_test_lemmatized = lemmatize(reviews_test)\n",
    "\n",
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(reviews_train_lemmatized)\n",
    "X = cv.transform(reviews_train_lemmatized)\n",
    "X_test = cv.transform(reviews_test_lemmatized)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "\n",
    "for c in [1, 0.5, 0.25, 0.05, 0.01]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy on train/validation set for C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                 accuracy_score(y_train, lr.predict(X_train)),\n",
    "                                                 accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train/test set: 0.980/0.874\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 0.25)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train/test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                       accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 63921)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher number of unique words as compared to stemming.\n",
    "<br>Stemming reduced the number of unique words from 71k to 48k, while lemmatization has reduced it to about 64k.\n",
    "<br>The accuracy is marginally reduced.\n",
    "<br>Lemmatization seems to be taking less computing time than stemming, which is a brute-force approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustration of a review after lemmatization\n",
      "Number of words: 137\n",
      "bromwell high is a cartoon comedy it ran at the same time a some other program about school life such a teacher my year in the teaching profession lead me to believe that bromwell high satire is much closer to reality than is teacher the scramble to survive financially the insightful student who can see right through their pathetic teacher pomp the pettiness of the whole situation all remind me of the school i knew and their student when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i here to sack one of your teacher student welcome to bromwell high i expect that many adult of my age think that bromwell high is far fetched what a pity that it is\n"
     ]
    }
   ],
   "source": [
    "print('Illustration of a review after lemmatization')\n",
    "print('Number of words:', len(reviews_train_lemmatized[0].split()))\n",
    "print(reviews_train_lemmatized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the effect of lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "as a\n",
      "programs program\n",
      "as a\n",
      "teachers teacher\n",
      "years year\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "\n",
    "for before, after in zip(reviews_train[0].split(), reviews_train_lemmatized[0].split()):\n",
    "    if after != before:\n",
    "        differences.append([before, after])\n",
    "\n",
    "print(len(differences))\n",
    "for i in range(5):\n",
    "    print(differences[i][0], differences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. N-GRAMS**\n",
    "\n",
    "Till now, we only used single word features in our model, which we call unigrams or 1-gram. The predictive power of our model can be potentially enhanced by adding two (bigrams) or three (trigrams) word sequences.\n",
    "\n",
    "For example, if we have the sequence 'didn't like the movie', considering single word features wont be able to capture that the review is negative, as the word 'like' will attribute the review to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set with C = 10.00: 1.000/0.888\n",
      "Accuracy on train/validation set with C = 5.00: 1.000/0.889\n",
      "Accuracy on train/validation set with C = 1.00: 1.000/0.888\n",
      "Accuracy on train/validation set with C = 0.10: 1.000/0.886\n",
      "Accuracy on train/validation set with C = 0.01: 0.968/0.877\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary = True, ngram_range = (1,2))\n",
    "cv.fit(reviews_train)\n",
    "X = cv.transform(reviews_train)\n",
    "X_test = cv.transform(reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "\n",
    "for c in [10, 5, 1, 0.1, 0.01]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy on train/validation set with C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                  accuracy_score(y_train, lr.predict(X_train)),\n",
    "                                                  accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train/test set: 1.000/0.897\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 1.00)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train/test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                       accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to note that just by including bigrams, in addition to the unigrams, has improved the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1433854)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique combinations have become order of magnitude larger (20 times). \n",
    "<br>Such humungous increase in number of features may cause overfitting, as evident in high accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. REPRESENTATIONS**\n",
    "\n",
    "Till now, we considered only binary representation of a review: if a word appears in the review, its corresponding entry in the column is 1 otherwise 0. \n",
    "<br>We can further improve the representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Word Counts**\n",
    "\n",
    "Instead of just representing whether a word exists in the review by 1, we can enter the number of times a particular word appears. For example, if the word 'amazing' appears three times, our entry in the word counts representation will be 3. This enhances the predictive power of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set with C = 1.00: 0.999/0.883\n",
      "Accuracy on train/validation set with C = 0.50: 0.996/0.888\n",
      "Accuracy on train/validation set with C = 0.25: 0.990/0.891\n",
      "Accuracy on train/validation set with C = 0.05: 0.960/0.895\n",
      "Accuracy on train/validation set with C = 0.01: 0.917/0.886\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary = False)\n",
    "cv.fit(reviews_train)\n",
    "X = cv.transform(reviews_train)\n",
    "X_test = cv.transform(reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "\n",
    "for c in [1, 0.5, 0.25, 0.05, 0.01]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy on train/validation set with C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                  accuracy_score(y_train, lr.predict(X_train)),\n",
    "                                                  accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train/test set: 0.958/0.881\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 0.05)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train/test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                       accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 71400)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. TF-IDF**\n",
    "\n",
    "Another popular way to represent each document (review) in a corpus is to use the tf-idf statistic (term frequency-inverse document frequency).\n",
    "<br> It is a weighting factor for the features (words here), that we can use in place of binary or word count representations.\n",
    "\n",
    "The tf-idf of a word in a document is given by:\n",
    " - tf-idf = term frequency * inverse document frequency\n",
    " - term frequency: number of times the word appears in the document\n",
    " - inverse document frequency: log(total number of documents/number of documents in which this particular word appears)\n",
    "\n",
    "\n",
    "Thus tf-idf\n",
    " - increases if the word appears multiple times in the document\n",
    " - decreases if it appears in multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set with C = 10.00: 0.989/0.892\n",
      "Accuracy on train/validation set with C = 5.00: 0.975/0.893\n",
      "Accuracy on train/validation set with C = 1.00: 0.929/0.888\n",
      "Accuracy on train/validation set with C = 0.10: 0.864/0.846\n",
      "Accuracy on train/validation set with C = 0.01: 0.796/0.794\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer()\n",
    "tv.fit(reviews_train)\n",
    "X = tv.transform(reviews_train)\n",
    "X_test = tv.transform(reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "\n",
    "for c in [10, 5, 1, 0.1, 0.01]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy on train/validation set with C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                 accuracy_score(y_train, lr.predict(X_train)),\n",
    "                                                 accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train/test set: 0.929/0.882\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 1)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train/test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                       accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 71400)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top positive and negative features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words\n",
      "('great', 7.5839160204097285)\n",
      "('excellent', 6.201453924200456)\n",
      "('best', 5.067651708523199)\n",
      "('perfect', 4.75066536258515)\n",
      "('wonderful', 4.612627848437834)\n",
      "--------------\n",
      "Negative words\n",
      "('worst', -9.35768375223538)\n",
      "('bad', -7.911932899691316)\n",
      "('waste', -6.360866543954613)\n",
      "('awful', -6.225442606697068)\n",
      "('boring', -5.834843690236099)\n"
     ]
    }
   ],
   "source": [
    "feature_coeff = {word: coeff for word, coeff in zip(tv.get_feature_names(), best_model.coef_[0])}\n",
    "\n",
    "print('Positive words')\n",
    "for best_positive in sorted(feature_coeff.items(), key = lambda x: x[1], reverse = True)[:5]:\n",
    "    print(best_positive)\n",
    "    \n",
    "print('--------------')\n",
    "   \n",
    "print('Negative words')    \n",
    "for best_negative in sorted(feature_coeff.items(), key = lambda x: x[1])[:5]:\n",
    "    print(best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that the top five discriminating words are not same, or in similar order as our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "In this notebook, we chose to represent each review as a very sparse vector, with a lots of zeroes. Linear classifiers typically perform better than other algorithms on data that is represented this way. Another algorithm that could work well is Support Vector Machine with linear kernel.\n",
    "\n",
    "We discussed several options for transforming text that can improve the accuracy of a NLP model. Which combinations of these techniques will yield the best results will depend on the task, data representation, and algorithms we choose. One should always try out many different combinations to see what works. \n",
    "\n",
    "\n",
    "As for now, lets combine few of the techniques we learnt to create a final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_processing(reviews_train_raw, reviews_test_raw):\n",
    "    \n",
    "    data_train = tokenize(reviews_train_raw)\n",
    "    data_test = tokenize(reviews_test_raw)\n",
    "    \n",
    "    data_train = remove_stop_words(data_train)\n",
    "    data_test = remove_stop_words(data_test)\n",
    "    \n",
    "    data_train = lemmatize(data_train)\n",
    "    data_test = lemmatize(data_test)\n",
    "    \n",
    "    cv = CountVectorizer(binary = False, ngram_range = (1,2))\n",
    "    cv.fit(data_train)\n",
    "    X = cv.transform(data_train)\n",
    "    X_test = cv.transform(data_test)\n",
    "    \n",
    "    return X, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = combined_processing(reviews_train_raw, reviews_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_model(X, target, clist):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.80)\n",
    "    \n",
    "    for c in clist:\n",
    "        \n",
    "        lr = LogisticRegression(C = c)\n",
    "        lr.fit(X_train, y_train)\n",
    "        print(\"Accuracy on train/validation set with C = %.2f: %.3f/%.3f\" % (c, \n",
    "                                                      accuracy_score(y_train, lr.predict(X_train)),\n",
    "                                                      accuracy_score(y_val, lr.predict(X_val))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train/validation set with C = 10.00: 1.000/0.889\n",
      "Accuracy on train/validation set with C = 5.00: 1.000/0.890\n",
      "Accuracy on train/validation set with C = 1.00: 1.000/0.889\n",
      "Accuracy on train/validation set with C = 0.10: 0.999/0.886\n",
      "Accuracy on train/validation set with C = 0.10: 0.999/0.886\n"
     ]
    }
   ],
   "source": [
    "clist = [10, 5, 1, 0.1, 0.1]\n",
    "search_best_model(X, target, clist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on train.test set: 1.000/0.882\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(C = 1.0)\n",
    "best_model.fit(X, target)\n",
    "print(\"Final accuracy on train.test set: %.3f/%.3f\" % (accuracy_score(target, best_model.predict(X)),\n",
    "                                                       accuracy_score(target, best_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after using regularization, we have less accuracy on the test set as compared to the training set, signalling overfitting. Thus, the hypothesis can be improved by collecting more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
